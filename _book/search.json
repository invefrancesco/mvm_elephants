[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MVM Elephants",
    "section": "",
    "text": "Questo documento contiene il codice e i risultati del progetto.\n\n\n\n\n\n\nWarning\n\n\n\nWork in progress",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>index.html</span>"
    ]
  },
  {
    "objectID": "code/vonmises_distribution.html",
    "href": "code/vonmises_distribution.html",
    "title": "2  Distribuzione Von Mises",
    "section": "",
    "text": "2.1 Introduzione\nIl post di Whittenbury Daniel contiene alcune note riguardanti la distribuzione von Mises.\nLa “statistica direzionale” ha numerosi usi nelle scienze\nLa distribuzione di von Mises è stata introdotta da Richard von Mises nel 1918 e la sua PDF è \\[\\begin{align}\n  f(x | \\mu, \\kappa) =  \\frac{\\exp(k \\cos(x - \\mu))}{2 \\pi I_0(k)}\n  \\end{align}\\]\nLe distribuzioni circolari hanno diversi vantaggi",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distribuzione Von Mises</span>"
    ]
  },
  {
    "objectID": "code/vonmises_distribution.html#introduzione",
    "href": "code/vonmises_distribution.html#introduzione",
    "title": "2  Distribuzione Von Mises",
    "section": "",
    "text": "La statistica classica, come sviluppata da Gauss, assume che gli errori di misura siano “piccoli” e distribuiti in uno spazio lineare infinito (come la retta dei numeri reali o uno spazio euclideo)\nQuesto funziona bene quando le misure sono molto precise come nel caso di astronomia o dei geometria che usano strumenti accurati\nLe misure angolari (come direzioni su una bussola, ore del giorno, orientamenti spaziali, ecc.) vivono naturalmente su una struttura topologica diversa (un cerchio o una sfera).\nSe gli errori diventano grandi (cioè, se i dati sono molto dispersi), non ha più senso ignorare la curvatura dello spazio su cui si trovano i dati.\nPer esempio, dire che la media tra 10° e 350° è 180° è assurdo in un contesto circolare, ma potrebbe sembrare corretto se si ignorasse la topologia.\n\n\n\nx è la variabile aleatoria angolare\n\\(\\mu\\) è la direzione media (con \\(\\mu \\in [0, 2 \\pi)\\))\n\\(\\kappa\\) è un parametro di concentrazione della distribuzione, che può assumere valori non negativi (\\(\\kappa \\in [0, \\infty)\\))\n\npiù è grande \\(\\kappa\\), più la distribuzione è concentrata attorno alla direzione media\n\n\\(I_0(\\kappa)\\) è la funzione di Bessel modificata del primo tipo e ordine zero\n\n\n\nsono definite su uno spazio campionario corretto\n\nla normale può produrre stime errate vicino ai limiti (es. 0 e \\(2\\pi\\))\n\nla distribuzione di von Mises ha un comportamento limite corretto quando \\(\\kappa \\to 0\\): tende a una distribuzione uniforme sul cerchio dove tutte le direzioni sono equiprobabili\nusare la normale in problemi circolari può portare a errori seri come fondere in modo scorretto una distribuzione a priori con una funzione di verosimiglianza\n\nnelle misurazioni dell’angolo di arrivo (in applicazioni di tracciamento), si rischia di aggiornare male la stima bayesiana se si ignora la natura ciclica della variabile",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distribuzione Von Mises</span>"
    ]
  },
  {
    "objectID": "code/vonmises_distribution.html#optim-function",
    "href": "code/vonmises_distribution.html#optim-function",
    "title": "2  Distribuzione Von Mises",
    "section": "2.2 Optim function",
    "text": "2.2 Optim function\nA common task in statistics is maximizing (or minimizing) complex univariate or multivariate functions. This is typically done in the context of maximizing likelihood functions with respect to a vector of unknown parameters, given the observed data.\n\nMore generally, this task pertains to an “optimization” problem, where we seek to finding a set of parameter values that minimizes or maximizes some pre-defined objective function.\nWhen this objective function is a log likelihood function, this reduces to the problem of maximum likelihood estimation.\nThere are many potential approaches to use for optimization in this context.\nHow best to optimize a function of interest depends on the nature of the function to be optimized, as well as practical concerns regarding a candidate procedure to be used.\n\nBy default optim performs minimization, but it will maximize if control$fnscale is negative. optimHess is an auxiliary function to compute the Hessian at a later stage if hessian = TRUE was forgotten.\n\nThe default method is an implementation of that of Nelder and Mead (1965), that uses only function values and is robust but relatively slow. It will work reasonably well for non-differentiable functions.\nMethod “BFGS” is a quasi-Newton method (also known as a variable metric algorithm), specifically that published simultaneously in 1970 by Broyden, Fletcher, Goldfarb and Shanno. This uses function values and gradients to build up a picture of the surface to be optimized.\nMethod “CG” is a conjugate gradients method based on that by Fletcher and Reeves (1964) (but with the option of Polak–Ribiere or Beale–Sorenson updates). Conjugate gradient methods will generally be more fragile than the BFGS method, but as they do not store a matrix they may be successful in much larger optimization problems.\nMethod “L-BFGS-B” is that of Byrd et al. (1995) which allows box constraints, that is each variable can be given a lower and/or upper bound. The initial value must satisfy the constraints. This uses a limited-memory modification of the BFGS quasi-Newton method. If non-trivial bounds are supplied, this method will be selected, with a warning.\nNocedal and Wright (1999) is a comprehensive reference for the previous three methods.\nMethod “SANN” is by default a variant of simulated annealing given in Belisle (1992).\nSimulated-annealing belongs to the class of stochastic global optimization methods. It uses only function values but is relatively slow.\nIt will also work for non-differentiable functions. This implementation uses the Metropolis function for the acceptance probability.\nBy default the next candidate point is generated from a Gaussian Markov kernel with scale proportional to the actual temperature. If a function to generate a new candidate point is given, method “SANN” can also be used to solve combinatorial optimization problems. - Temperatures are decreased according to the logarithmic cooling schedule as given in Belisle (1992, p. 890); specifically, the temperature is set to temp / log(((t-1) %/% tmax) * tmax + exp(1)), where t is the current iteration step and temp and tmax are specifiable via control, see below. - Note that the “SANN” method depends critically on the settings of the control parameters. It is not a general-purpose method but can be very useful in getting to a good value on a very rough surface. - Method “Brent” is for one-dimensional problems only, using optimize(&lt;ff&gt;, lower, upper, tol = control$reltol) where &lt;ff&gt; is function(par) fn(par, ...)/control$fnscale. It can be useful in cases when optim() is used inside other functions where only method can be specified, such as in mle from package stats4.\n\nFunction fn can return NA or Inf if the function cannot be evaluated at the supplied value, but the initial value must have a computable finite value of fn. (Except for method “L-BFGS-B” where the values should always be finite.)\noptim can be used recursively, and for a single parameter as well as many. It also accepts a zero-length par, and just evaluates the function with that argument",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distribuzione Von Mises</span>"
    ]
  },
  {
    "objectID": "code/vonmises_distribution.html#stima-di-una-von-mises-con-una-covariata",
    "href": "code/vonmises_distribution.html#stima-di-una-von-mises-con-una-covariata",
    "title": "2  Distribuzione Von Mises",
    "section": "2.3 Stima di una Von Mises con una covariata",
    "text": "2.3 Stima di una Von Mises con una covariata\nCreo due vettori \\(\\mathbf{x}_n\\) e \\(\\mathbf{y}_n\\) in cui\n\n\\(X_i \\sim N(0, 1)\\)\n\\(Y_i \\sim VonMises(\\mu = 2atan(\\beta_0 +\\beta_1 \\cdot x_i), k = 10)\\) con \\(\\beta_0=-1\\), \\(\\beta_1 = 3.5\\)\n\n\n\nCode\nrndm_x &lt;- rnorm(1000)\nrndm_y &lt;- sapply(rndm_x, function(xi) circular::rvonmises(n = 1, mu = 2*atan(-1 + 3.5 * xi), kappa = 10))\n\n\nDefinisco la funzione di - log-verosimiglianza\n\n\nCode\nlog.lik.vm &lt;- function(par, x, y){\n  beta.0 &lt;- par[1]\n  beta.1 &lt;- par[2]\n  kappa &lt;- exp(par[3])\n  \n  l &lt;- array(dim = length(x))\n  \n  for(i in seq_along(x)){\n    l[i] = kappa * cos(y[i] - 2 * atan(beta.0 + beta.1*x[i])) - log((besselI(kappa, nu = 0)))\n  }\n  \n  return(-sum(l))\n}\n\n\nMinimizzo la funione con optim\n\n\nCode\nresult &lt;- optim(\n  par = c(beta.0 = 0, beta.1 = 0, kappa = log(2)),\n  fn = log.lik.vm,\n  x = rndm_x,\n  y = rndm_y, \n  hessian = T)\n\n# Standard error\ndet(result$hessian) != 0 # condizione di invertibilità\n\n\n[1] TRUE\n\n\nCode\ncov.matrix &lt;- solve(result$hessian)\nse &lt;- sqrt(diag(cov.matrix))\n\n# Intervalli di confidenza asintotici (95%)\ntibble(\n  parametri = c(\"beta 0\", \"beta 1\", \"kappa\"), \n  estimate = result$par,\n  se = se,\n  lower = estimate - qnorm(0.975) * se,\n  upper = estimate + qnorm(0.975) * se\n) %&gt;% \n  kable(\n    col.names = c(\"Parameter\", \"Estimate\", \"Std. Error\", \"95% CI Lower\", \"95% CI Upper\"),\n    align = \"lcccc\",\n    format = \"html\", \n    digits = 3\n  )\n\n\n\n\n\nParameter\nEstimate\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\nbeta 0\n-0.997\n0.018\n-1.033\n-0.961\n\n\nbeta 1\n3.561\n0.055\n3.454\n3.668\n\n\nkappa\n2.325\n0.043\n2.239\n2.410\n\n\n\n\n\nNota esplicativa:\n\nIl parametro kappa è stimato in scala logaritmica per garantire che il valore restituito sia sempre positivo. Il valore par[3] rappresenta quindi il logaritmo naturale del parametro reale, e viene esponenziato all’interno della funzione di verosimiglianza.\nImpostando hessian = TRUE, optim() restituisce anche la matrice Hessiana della funzione obiettivo valutata nel punto di minimo, che corrisponde (sotto opportune condizioni regolarità) alla matrice di informazione osservata.\nSe questa matrice Hessiana è invertibile, la sua inversa fornisce una stima asintotica della matrice di varianza-covarianza dei parametri stimati.\nLe radici quadrate degli elementi diagonali della matrice inversa corrispondono agli errori standard asintotici delle stime dei parametri.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distribuzione Von Mises</span>"
    ]
  },
  {
    "objectID": "code/vonmises_distribution.html#simulazione-di-k-repliche-per-lo-stesso-set-di-parametri-veri",
    "href": "code/vonmises_distribution.html#simulazione-di-k-repliche-per-lo-stesso-set-di-parametri-veri",
    "title": "2  Distribuzione Von Mises",
    "section": "2.4 Simulazione di \\(K\\) repliche per lo stesso set di parametri veri",
    "text": "2.4 Simulazione di \\(K\\) repliche per lo stesso set di parametri veri\nGenero \\(K\\) set di dati random e stimo i parametri che massimizzano la verosimiglianza per ogni set di dati\n\n\nCode\ngen.data &lt;- function(n, beta.0, beta.1, kappa){\n  x &lt;- rnorm(n)\n  y &lt;- sapply(x, function(xi) circular::rvonmises(n= 1, mu= 2*atan(beta.0 + beta.1 * xi), kappa = kappa))\n  data &lt;- tibble(x=x, y=y)\n}\n\n\nPer riproducibilità genero una sola volta una lista di \\(K = 1000\\) tibble inserendo come come parametri n = 1000, beta.0 = -1, beta.1 = 3.5, kappa = 10 e la salvo in un file chiamato sim_1.RData\n\n\nCode\nload(paste0(dir_data, \"/sim_1.RData\"))\n\nresults &lt;- map_dfr(\n  .x = sim_1,\n  .f = ~ {\n    mle &lt;- optim(\n      par = c(beta.0 = 0, beta.1 = 0, kappa = log(2)),\n      fn = log.lik.vm,\n      x = .x$x,\n      y = .x$y\n    )\n    \n    tibble(\n      beta.0 = mle$par[1],\n      beta.1 = mle$par[2],\n      kappa  = exp(mle$par[3]),\n      convergence = mle$convergence\n    )\n  }\n)\n\n# Visualizzazione ----\nsummary_df &lt;- tibble(\n  Parameter = c(\"beta 0\", \"beta 1\", \"kappa\"),\n  Mean      = c(mean(results$beta.0), mean(results$beta.1), mean(results$kappa)),\n  `Std. Dev` = c(sd(results$beta.0), sd(results$beta.1), sd(results$kappa)),\n  `2.5%`    = c(quantile(results$beta.0, 0.025),\n                quantile(results$beta.1, 0.025),\n                quantile(results$kappa, 0.025)),\n  `97.5%`   = c(quantile(results$beta.0, 0.975),\n                quantile(results$beta.1, 0.975),\n                quantile(results$kappa, 0.975))\n)\n\nsummary_df %&gt;%\n  mutate(across(where(is.numeric), ~ round(.x, 4))) %&gt;%\n  kable(\n    align = \"lcccc\",\n    col.names = c(\"Parameter\", \"Mean\", \"Std. Dev\", \"2.5%\", \"97.5%\"),\n    format = \"markdown\"\n  )\n\n\n\n\n\nParameter\nMean\nStd. Dev\n2.5%\n97.5%\n\n\n\n\nbeta 0\n-1.0001\n0.0198\n-1.0386\n-0.9616\n\n\nbeta 1\n3.5010\n0.0558\n3.3933\n3.6093\n\n\nkappa\n10.0305\n0.4347\n9.2221\n10.9423",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Distribuzione Von Mises</span>"
    ]
  },
  {
    "objectID": "code/preparazione_dati.html",
    "href": "code/preparazione_dati.html",
    "title": "3  Preparazione dei dati",
    "section": "",
    "text": "3.1 Introduzione\nSe sono disponibili dati sulla posizione degli animali registrata a intervalli regolare, per analizzarne il movimento degli animali vengono utilizzate tre metriche:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparazione dei dati</span>"
    ]
  },
  {
    "objectID": "code/preparazione_dati.html#introduzione",
    "href": "code/preparazione_dati.html#introduzione",
    "title": "3  Preparazione dei dati",
    "section": "",
    "text": "La step length, misurata come distanza euclidea tra due rilevazioni successive.\n\n\n\n\n\n\n\nNote\n\n\n\nSe la posizione è registrata in gradi, anche la distanza euclidea sarà in gradi. È opportuno in questo caso convertire le coordinate in un sistema di proiezione metrica.\n\n\n\nLa direction, misurata come atan2, espressa in radianti e compresa tra \\(-\\pi\\) e \\(\\pi\\). Lo 0 indica l’est geografico.\nIl turn angle misurato come differenza tra direction (cioè atan2) successivi.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparazione dei dati</span>"
    ]
  },
  {
    "objectID": "code/preparazione_dati.html#verifiche-sul-pacchetto-amt",
    "href": "code/preparazione_dati.html#verifiche-sul-pacchetto-amt",
    "title": "3  Preparazione dei dati",
    "section": "3.2 Verifiche sul pacchetto amt",
    "text": "3.2 Verifiche sul pacchetto amt\nPer la preparazione dei dati verrà utilizzato il pacchetto amt [@signer_animal_nodate]: è fondamentale in via preliminare verificare che i calcoli che avvengono all’interno del pacchetto corrispondano a quelli descritti sopra. Per farlo, vengono creati manualmente dei semplici dati di input e si calcolano le metriche analiticamente.\n\n\nCode\n# dati di input --- \ndf &lt;- tibble(\n  x = c(0, 1, 1, 0),\n  y = c(0, 0, 1, 1),\n  t = as.POSIXct(c(\"2025-01-01 00:00:00\", \"2025-01-01 01:00:00\",\n                   \"2025-01-01 02:00:00\", \"2025-01-01 03:00:00\"))\n)\n# plot ---\nggplot(df, aes(x, y)) +\n  geom_path() +\n  geom_point() +\n  coord_equal() + \n  theme_test()\n\n\n\n\n\n\n\n\n\nCode\n# calcoli manuali ---\ntibble(\n  step_length = c(\n    sqrt((df$x[2] - df$x[1])^2 + (df$y[2] - df$y[1])^2), \n    sqrt((df$x[3] - df$x[2])^2 + (df$y[3] - df$y[2])^2),\n    sqrt((df$x[4] - df$x[3])^2 + (df$y[4] - df$y[3])^2)\n  ),\n  direction = c(\n    atan2(df$y[2], df$x[2]),\n    atan2(df$y[3], df$x[3]),\n    atan2(df$y[4], df$x[4])\n  ), \n  turn_angle = c(\n    NA, \n    direction[2] - direction[1],\n    direction[3] - direction[2]\n  )\n) %&gt;% \n  kable(\n    col.names = c(\"Step Length\", \"Direction\", \"Turn angle\"),\n    format = \"html\", \n    digits = 3\n  )\n\n\n\n\n\nStep Length\nDirection\nTurn angle\n\n\n\n\n1\n0.000\nNA\n\n\n1\n0.785\n0.785\n\n\n1\n1.571\n0.785\n\n\n\n\n\nI risultati ottenuti possono ora essere confrontati con quelli ottenuti tramite il pacchetto.\n\n\nCode\n# preparazione dei dati ----\ntrk &lt;- make_track(df, x, y, t) # sistema proiettato (UTM zona 32N)\nsteps &lt;- trk %&gt;% \n  amt::track_resample(rate = hours(1)) %&gt;% \n  steps_by_burst()\n# risultati ----\nsteps %&gt;% \n  dplyr::select(sl_, direction_p, ta_) %&gt;%\n  kable(\n    col.names = c(\"Step Length\", \"Direction\", \"Turn angle\"),\n    format = \"html\",\n    digits = 3\n  )\n\n\n\n\n\nStep Length\nDirection\nTurn angle\n\n\n\n\n1\n0.000\nNA\n\n\n1\n1.571\n1.571\n\n\n1\n3.142\n1.571\n\n\n\n\n\nUn altro elemento essenziale da comprendere, soprattutto nell’ottica dell’analisi dei residui, è il segno del turn angle\n\nSe il segno del turn angle è negativo vuol dire che l’animale ha girato in senso orario\nPer verificarlo e visualizzarlo creo un percorso in cui la step length è fissa (\\(=1\\)) e l’animale si muove in quattro direzioni (\\(\\pi/4, 0, -\\pi/4, 0\\)) e in cui ci aspettiamo che i primi due turn angle siano negativi mentre il terzo e il quarto siano positivi\n\n\n\nCode\n# direzioni ----\ndirection &lt;- c(pi/4, 0, -pi/4, 0, pi/4)\n# coordinate cumulative della traiettoria ----\nx &lt;- cumsum(c(0, cos(direction)))\ny &lt;- cumsum(c(0, sin(direction)))\n# df con le coordinate e gli angoli ----\ndf &lt;- tibble(\n  step = 0:5,\n  x = x,\n  y = y,\n  direction = c(NA, direction),\n  ta = c(NA, diff(direction))\n)\ndf %&gt;% \n  kable(\n    col.names = c(\"Step\", \"x\", \"y\", \"Direction\", \"Turn angle\"),\n    format = \"markdown\", \n    digits = 3\n  )\n\n\n\n\n\nStep\nx\ny\nDirection\nTurn angle\n\n\n\n\n0\n0.000\n0.000\nNA\nNA\n\n\n1\n0.707\n0.707\n0.785\nNA\n\n\n2\n1.707\n0.707\n0.000\n-0.785\n\n\n3\n2.414\n0.000\n-0.785\n-0.785\n\n\n4\n3.414\n0.000\n0.000\n0.785\n\n\n5\n4.121\n0.707\n0.785\n0.785\n\n\n\n\n\nCode\n# plot ----\nggplot(df, aes(x, y)) +\n  geom_path(linewidth = 1, linetype = 2) +\n  geom_point() +\n  coord_equal() +\n  theme_test()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparazione dei dati</span>"
    ]
  },
  {
    "objectID": "code/preparazione_dati.html#preparazione-dei-dati-id-elefante-gon09",
    "href": "code/preparazione_dati.html#preparazione-dei-dati-id-elefante-gon09",
    "title": "3  Preparazione dei dati",
    "section": "3.3 Preparazione dei dati (id elefante: GON09)",
    "text": "3.3 Preparazione dei dati (id elefante: GON09)\nLe principali funzioni per la preparazione dei dati sono le seguenti:\n\namt::make_track() crea un “track” cioè una tibble contenente le coordinate x e y, il tempo (t) e un sistema di riferimento di coordinate (CRS). Questa tibble verrà utilizzata dal pacchetto per le operazioni successive. Nel nostro caso il CRS è identificato con EPSG:21036, Arc 1960, UTM zone 36S.\n\n\n\nCode\nload(paste0(dir_data, \"/data_elephants_clean_envcovs.RData\"))\nGON09 &lt;- data_elephants %&gt;% #ordino i dati per id e data/ora\n  arrange(id, timepoint) %&gt;% \n  filter(id == \"GON09\")\ncrs &lt;- capture.output(st_crs(GON09))\n\n\n\namt::summarize_sampling_rate() fornisce la distribuzione della frequenza di campionamento e consente di vedere se le registrazione della posizione sono state costanti e con che intervallo.\n\n\n\nCode\nGON09_trk &lt;- amt::make_track(GON09,\n                             .x = \"XUTM\",\n                             .y = \"YUTM\", \n                             .t = \"timepoint\",\n                             crs = 21036) \namt::summarize_sampling_rate(GON09_trk) %&gt;% \n  kable(format = \"html\")\n\n\n\n\n\nmin\nq1\nmedian\nmean\nq3\nmax\nsd\nn\nunit\n\n\n\n\n0.0002778\n3.994722\n4.000556\n4.219743\n4.006667\n296.0897\n4.759062\n4831\nhour\n\n\n\n\n\n\namt::track_resample() unito a filter_min_n_burst(3) permettono di conservare le osservazioni solo nel caso in cui in cui le registrazioni della posizione avvengono ad un intervallo costante (nel nostro caso 4h \\(\\pm\\) 15 min) per almeno tre registrazioni consecutive\nsteps_by_burst() passa da un df in cui ogni riga corrisponde ad una posizione ad uno in cui ogni riga corrisponde ad un passo. Per ogni passo, sono incluse le variabili coordinate di inizio (x1_, y1_ ), cordinate finali (x2_, y2_ ), tempo di inizio e di fine (t1_, t2_ ), step length (sl_; in CRS units), turning angles (ta_; in degrees), la time difference (dt_ ) e la burst (sequenza di passi con intervallo di registrazione di 4h) (burst_ ) a cui appartiene il step\n\n\n\nCode\nGON09_trk &lt;-  track_resample(GON09_trk,\n                             rate = hours(4), \n                             tolerance = minutes(20)) %&gt;% \n  filter_min_n_burst(3) %&gt;% \n  steps_by_burst()\n\n\n\nL’ultimo passaggio per la preparazione dei dati consiste nell’associazione delle covariate ambientali agli spostamenti dell’animale. Abbiamo a disposizione, per ogni posizione, la distanza dalla riva (distriv), l’indice NDVI (ndvi), l’elevazione (elev) e la stagione (seas). Quest’ultima è una variabile con tre livelli: hot wet (HW) (Da Novembre a Marzo, quando si verifica il 90% delle piogge annuali); cool dry (CD) (Da aprile ad agosto) e hot dry (HD) (da Settembre a Ottobre) [@mandinyenya_sex_2024]\nÈ importante notare che il fatto che le covariate siano associate al punto di arrivo e non al punto di partenza, modifica sostanzialmente la domanda di ricerca. Cioè si investiga in questo modo quale sia la relazione tra il modo in cui si muove l’animale e il luogo verso il quale sta andando.\n\n\n\nCode\nGON09_fnl &lt;- GON09_trk %&gt;% \n  left_join(\n    dplyr::select(GON09, XUTM, YUTM, distriv, ndvi, elev, seas),\n    join_by(\"x2_\" == \"XUTM\", \"y2_\" == \"YUTM\")\n  )\nknitr::kable(head(GON09_fnl, 15), format = \"markdown\", digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nburst_\nx1_\nx2_\ny1_\ny2_\nsl_\ndirection_p\nta_\nt1_\nt2_\ndt_\ndistriv\nndvi\nelev\nseas\ngeometry\n\n\n\n\n3\n414.44\n414.83\n-2338.37\n-2338.32\n0.39\n0.14\nNA\n2016-02-02 01:58:49\n2016-02-02 05:57:46\n3.98 hours\n1.21\n2564\n520\nHW\nPOINT (414.8288 -2338.318)\n\n\n3\n414.83\n414.83\n-2338.32\n-2339.04\n0.72\n-1.58\n-1.72\n2016-02-02 05:57:46\n2016-02-02 09:57:57\n4.00 hours\n0.81\n2551\n514\nHW\nPOINT (414.8256 -2339.035)\n\n\n3\n414.83\n416.14\n-2339.04\n-2339.60\n1.43\n-0.41\n1.17\n2016-02-02 09:57:57\n2016-02-02 13:57:52\n4.00 hours\n0.47\n3178\n508\nHW\nPOINT (416.1403 -2339.6)\n\n\n5\n415.06\n414.39\n-2340.00\n-2339.49\n0.84\n2.49\nNA\n2016-02-03 13:57:54\n2016-02-03 17:58:00\n4.00 hours\n0.69\n2677\n507\nHW\nPOINT (414.3919 -2339.493)\n\n\n5\n414.39\n413.71\n-2339.49\n-2338.76\n1.00\n2.32\n-0.17\n2016-02-03 17:58:00\n2016-02-03 21:58:02\n4.00 hours\n1.69\n2512\n522\nHW\nPOINT (413.7098 -2338.764)\n\n\n5\n413.71\n414.15\n-2338.76\n-2338.21\n0.70\n0.90\n-1.42\n2016-02-03 21:58:02\n2016-02-04 01:58:02\n4.00 hours\n1.88\n3931\n504\nHW\nPOINT (414.1464 -2338.214)\n\n\n5\n414.15\n415.03\n-2338.21\n-2337.89\n0.94\n0.36\n-0.54\n2016-02-04 01:58:02\n2016-02-04 05:57:59\n4.00 hours\n1.26\n3120\n507\nHW\nPOINT (415.0256 -2337.887)\n\n\n5\n415.03\n415.18\n-2337.89\n-2338.44\n0.58\n-1.29\n-1.65\n2016-02-04 05:57:59\n2016-02-04 09:58:05\n4.00 hours\n0.84\n2634\n514\nHW\nPOINT (415.1842 -2338.441)\n\n\n5\n415.18\n413.26\n-2338.44\n-2340.91\n3.13\n-2.23\n-0.94\n2016-02-04 09:58:05\n2016-02-04 13:58:10\n4.00 hours\n1.34\n3681\n501\nHW\nPOINT (413.2572 -2340.906)\n\n\n5\n413.26\n412.03\n-2340.91\n-2341.73\n1.48\n-2.55\n-0.32\n2016-02-04 13:58:10\n2016-02-04 17:58:13\n4.00 hours\n1.78\n3174\n471\nHW\nPOINT (412.0313 -2341.73)\n\n\n5\n412.03\n410.50\n-2341.73\n-2343.37\n2.24\n-2.32\n0.23\n2016-02-04 17:58:13\n2016-02-04 21:58:07\n4.00 hours\n0.22\n2394\n437\nHW\nPOINT (410.5037 -2343.371)\n\n\n5\n410.50\n410.27\n-2343.37\n-2343.08\n0.38\n2.24\n-1.73\n2016-02-04 21:58:07\n2016-02-05 01:58:16\n4.00 hours\n0.57\n2128\n440\nHW\nPOINT (410.2719 -2343.075)\n\n\n5\n410.27\n410.52\n-2343.08\n-2342.77\n0.40\n0.90\n-1.34\n2016-02-05 01:58:16\n2016-02-05 05:58:04\n4.00 hours\n0.78\n2735\n445\nHW\nPOINT (410.5176 -2342.766)\n\n\n5\n410.52\n411.15\n-2342.77\n-2342.62\n0.65\n0.22\n-0.68\n2016-02-05 05:58:04\n2016-02-05 10:00:56\n4.05 hours\n0.86\n2881\n451\nHW\nPOINT (411.1467 -2342.622)\n\n\n8\n414.42\n413.38\n-2340.33\n-2338.81\n1.83\n2.17\nNA\n2016-02-06 17:58:25\n2016-02-06 21:58:27\n4.00 hours\n1.88\n2512\n523\nHW\nPOINT (413.3846 -2338.814)\n\n\n\n\n\nCode\nwrite_csv(GON09_fnl, paste0(dir_data, \"/GON09_fnl.csv\"))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Preparazione dei dati</span>"
    ]
  },
  {
    "objectID": "code/GON09.html",
    "href": "code/GON09.html",
    "title": "4  Analisi del primo elefante (id: GON09)",
    "section": "",
    "text": "4.1 Analisi esplorativa dei dati\nGli istogrammi rappresentano le distribuzioni di step length, turn angle e direction nelle tre stagioni climatiche\nCode\n# data \nGON09_fnl &lt;- read_csv(paste0(dir_data, \"/GON09_fnl.csv\"))\n# step length ----\nggplot(data = GON09_fnl, aes(x = sl_, y = ..density..)) +\n  geom_histogram(color = \"black\", fill = NA) +\n  geom_density() +\n  facet_wrap(~ seas) +\n  theme_test() +\n  xlab(\"Step length [km]\") +\n  ylab(\"Density\")\n\n\n\n\n\n\n\n\n\nCode\n# turn angle\nggplot(data = GON09_fnl, aes(x = ta_, y = ..density..)) +\n  geom_histogram(color = \"black\", fill = NA) +\n  geom_density() +\n  facet_wrap(~ seas) +\n  theme_test() +\n  xlab(\"Turn angle\") +\n  ylab(\"Density\")\n\n\n\n\n\n\n\n\n\nCode\n# direction\nggplot(data = GON09_fnl, aes(x = direction_p, y = ..density..)) +\n  geom_histogram(color = \"black\", fill = NA) +\n  geom_density() +\n  facet_wrap(~ seas) +\n  theme_test() +\n  xlab(\"Direction\") +\n  ylab(\"Density\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Analisi del primo elefante (id: GON09)</span>"
    ]
  },
  {
    "objectID": "code/GON09.html#modello-di-base",
    "href": "code/GON09.html#modello-di-base",
    "title": "4  Analisi del primo elefante (id: GON09)",
    "section": "4.2 Modello di base",
    "text": "4.2 Modello di base\nVogliamo verificare se il turn angle sia influenzato dalle variabili a disposizione, cioè la distanza dalla riva distriv, l’altitudine elev, l’indice ndvi e la stagione seas.\n\nLa funzione di verosimiglianza è definita assumendo\n\n\\[\n\\boldsymbol{\\mu} = 2 \\cdot \\arctan(\\mathbf{X} \\boldsymbol{\\beta}) = 2 \\cdot \\arctan(\\eta_i)\n\\]\ndove:\n\n\\(\\mathbf{X} = (1, \\text{distriv}, \\text{elev}, \\text{ndvi}, \\text{seas})\\)\n\\(\\boldsymbol{\\beta} = (\\beta_0, \\beta_1, \\dots, \\beta_k)\\)\nIn altri termini, assumiamo per il turn angle (ta) il seguente modello:\n\n\\[\ny_i \\propto \\beta_0 + \\beta_1 \\cdot \\text{distriv}_i + \\beta_2 \\cdot \\text{elev}_i +\n\\beta_3 \\cdot \\text{ndvi}_i + \\beta_4 \\cdot \\text{seasCD}_i + \\beta_5 \\cdot \\text{seasHD}_i\n\\]\n\nLa variabile seas è trattata come fattore con base la stagione “hot dry” (HD);\nLe covariate sono standardizzate per migliorare l’interpretabilità dei coefficienti stimati.\n\n\n\nCode\n# funzione di log-verosimiglianza ----\nlog.lik.VM &lt;- function(par, data, formula, response){\n  # Dati\n  X &lt;- model.matrix(formula, data = data)\n  y &lt;- data[[response]]\n  \n  # Parametri\n  p &lt;- ncol(X)\n  beta &lt;- par[1:p]\n  kappa &lt;- exp(par[p+1])\n  \n  # Elimina righe con NA\n  valid &lt;- complete.cases(y, X)\n  y &lt;- y[valid]\n  X &lt;- X[valid, , drop = FALSE]\n  \n  # Link function\n  eta &lt;- X %*% beta\n  mu &lt;- 2 * atan(eta)\n  \n  # Log-likelihood function\n  l &lt;- kappa * cos(y - mu) - log(besselI(kappa, nu = 0))\n  return(-sum(l))\n}\n# standardizzo le covariate ---- \nGON09_fnl &lt;- GON09_fnl %&gt;% \n  mutate(\"distriv_std\" = scale(GON09_fnl$distriv)[,1],\n         \"elev_std\" = scale(GON09_fnl$elev)[,1],\n         \"ndvi_std\" = scale(GON09_fnl$ndvi)[,1])\n# fit ----\nfit_std &lt;- optim(\n  par = c(rep(0,6), log(2)),\n  fn = log.lik.VM, \n  data = GON09_fnl, \n  formula = ~ distriv_std + elev_std + ndvi_std + seas, \n  response = \"ta_\",\n  hessian = T\n)\n# risultati ----\n#     tabella ----\ntibble(\n  parametri = c(\"Intercept\", \"Distance from water\", \"Elevation\", \"NDVI Index\", \"Seas = CD\", \"Seas = HD\", \"Kappa\"),\n  estimate = c(fit_std$par[1:6], exp(fit_std$par[7])), \n  se = sqrt(diag(solve(fit_std$hessian))),\n  lower = estimate - qnorm(0.975) * se,\n  upper = estimate + qnorm(0.975) * se,\n  W = estimate / se, \n  p_value = 2 * (1 - pnorm(abs(W)))\n) %&gt;% \n  mutate(across(where(is.numeric), ~ round(.x, 4))) %&gt;%\n  kable(\n    col.names = c(\"Parameter\", \"Estimate\", \"Std. Error\", \"95% CI Lower\", \"95% CI Upper\", \"Wald test\", \"p-value\"),\n    align = \"lcccc\",\n    format = \"markdown\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStd. Error\n95% CI Lower\n95% CI Upper\nWald test\np-value\n\n\n\n\nIntercept\n-0.0387\n0.0245\n-0.0866\n0.0093\n-1.5803\n0.1140\n\n\nDistance from water\n0.0726\n0.0157\n0.0418\n0.1034\n4.6152\n0.0000\n\n\nElevation\n0.0249\n0.0155\n-0.0055\n0.0554\n1.6040\n0.1087\n\n\nNDVI Index\n0.0500\n0.0164\n0.0178\n0.0822\n3.0455\n0.0023\n\n\nSeas = CD\n-0.0006\n0.0365\n-0.0722\n0.0710\n-0.0170\n0.9864\n\n\nSeas = HD\n-0.0650\n0.0358\n-0.1352\n0.0052\n-1.8154\n0.0695\n\n\nKappa\n0.7657\n0.0319\n0.7033\n0.8282\n24.0291\n0.0000\n\n\n\n\n\nCode\n#     grafico dei coefficienti ----\ntibble(\n  parameter = factor(c(\"Intercept\", \"Distance from water\", \"Elevation\", \"NDVI Index\",\n                       \"Seas = CD\", \"Seas = HD\", \"Kappa\"),\n                     levels = c(\"Intercept\", \"Distance from water\", \"Elevation\", \"NDVI Index\",\n                                \"Seas = CD\", \"Seas = HD\", \"Kappa\")),\n  estimate = c(fit_std$par[1:6], exp(fit_std$par[7])), \n  se = sqrt(diag(solve(fit_std$hessian))),\n  lower = estimate - qnorm(0.975) * se,\n  upper = estimate + qnorm(0.975) * se\n) %&gt;%\n  ggplot(aes(estimate, parameter)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = lower, xmax = upper)) +\n  geom_vline(xintercept = 0, lty = 2, color = \"red\") +\n  labs(\n    x = \"Estimate with conf. intervals\"\n  ) +\n  theme_test()\n\n\n\n\n\n\n\n\n\nCode\n#       curve di regressione al variare di distriv ----\nexpand_grid(\n  distriv_seq = seq(\n    from = -10,\n    to = 10,\n    length.out = 100\n  ),\n  seas = c(\"HW\", \"CD\", \"HD\")) %&gt;% \n  mutate(\n    seasCD = as.numeric(seas == \"CD\"),\n    seasHD = as.numeric(seas == \"HD\"),\n    mu = 2 * atan(fit_std$par[1] + distriv_seq * fit_std$par[2] + seasCD * fit_std$par[5] + seasHD * fit_std$par[6])\n  ) %&gt;% \n  ggplot(aes(x = distriv_seq, y = mu, color = seas)) +\n  geom_line() +\n  labs(\n    x = \"Distance from water\",\n    y = \"Turn angle\"\n  ) + \n  scale_y_continuous(limits = c(-pi, pi)) +\n  theme_test()\n\n\n\n\n\n\n\n\n\n\n4.2.1 Analisi dei residui\nI residui (\\(res = y^{oss} - \\hat{y}\\)) sono sempre compresi in \\(-\\pi\\) a \\(+\\pi\\), cioè circa tra \\(-3.14\\) e $+3.14 assumono i seguenti valori:\n\n\\(&gt; 0\\) se \\(y^{oss} &gt; \\hat{y}\\) cioè quando gira più a sinistra (senso antiorario) rispetto al previsto\n\\(&lt; 0\\) se \\(y^{oss} &lt; \\hat{y}\\) cioè quando gira più a destra (senso orario) rispetto al previsto\n\nSi vuole verificare se i residui al tempo \\(t\\) siano correlati con i residui al tempo \\(t-1, \\dots, t - n\\)\n\nTrattandosi di dati angolari il coefficiente di correlazione è definito come [see @fisher_statistical_2000]:\n\n\\[\n\\rho_c(\\alpha, \\beta) = \\frac{E\\{ sin(\\alpha - \\mu) sin(\\beta - \\nu) \\}}{\\sqrt{\\text{Var}(sin(\\alpha - \\mu)) \\text{Var}(sin(\\beta - \\nu)) }}\n\\]\nUn problema che si incontra con questo dataset riguarda il fatto che non tutte le osservazioni sono ad intervalli regolari tra di loro\n\nLe osservazioni della stessa burst hanno una differenza di quattro ore tra una e l’altra, ma tra una burst e un’altra la distanza temporale è molto superiore\nPer ovviare a questo problema la funzione cerca un match tra l’orario desiderato e un’orario osservato nei dati\n\n\n\nCode\nresiduals &lt;- GON09_fnl %&gt;%\n  arrange(t1_) %&gt;%\n  mutate(\n    seasCD = as.numeric(seas == \"CD\"),\n    seasHD = as.numeric(seas == \"HD\"),\n    ta_hat = 2 * atan(fit_std$par[1] + \n                        fit_std$par[2] * GON09_fnl$distriv_std + \n                        fit_std$par[3] *  GON09_fnl$elev_std+ \n                        fit_std$par[4] * GON09_fnl$ndvi_std+ \n                        fit_std$par[5] * seasCD + \n                        fit_std$par[6] * seasHD)\n  ) %&gt;% \n  dplyr::select(t2_, ta_, ta_hat) %&gt;% \n  dplyr::filter(!is.na(ta_)) %&gt;% \n  mutate(\n    res = (ta_ - ta_hat),\n    t2_ = round_date(t2_, unit = \"hours\")\n  ) %&gt;% \n  select(t2_, res)\n# funzione per calcolare ACF ----\nacf_circular &lt;- function(lag, x){\n  res_lag &lt;- x %&gt;%  \n    mutate(t2_lag = t2_ + hours(lag * 4))\n  \n  res_lag &lt;- res_lag %&gt;% \n    inner_join(\n      residuals,\n      by = c(\"t2_lag\" = \"t2_\")\n    )\n  \n  n_match &lt;- nrow(res_lag)\n  \n  cor &lt;-  circular::cor.circular(res_lag$res.x, res_lag$res.y, test = T)\n  tibble(\n    lag = lag,\n    n_match = n_match,\n    acf = cor[[1]],\n    statistic = cor[[2]],\n    p.value = cor[[3]]\n  )\n}\n# calcolo ACF ----\nACF &lt;- map_dfr(\n  .x = 1:42, \n  .f = ~acf_circular(lag = .x, x = residuals)\n) \n# risultati ----\nACF %&gt;%\n  mutate(\n    total_hours = lag * 4,\n    days = floor(total_hours / 24),\n    hours = total_hours %% 24,\n    lag_label = case_when(\n      days &gt; 0 & hours &gt; 0 ~ paste0(days, \" days \", hours, \" hours\"),\n      days &gt; 0 & hours == 0 ~ paste0(days, \" days\"),\n      days == 0 & hours &gt; 0 ~ paste0(hours, \" hours\"),\n      TRUE ~ \"0 hours\"\n    ),\n    signif = ifelse(p.value &lt; 0.05, \"*\", \"\")\n  ) %&gt;% \n  select(lag_label, n_match, acf, statistic, p.value, signif) %&gt;%\n  kable(format = \"markdown\",\n        col.names = c(\"Lag\", \"# of matches\", \"ACF\", \"Statistic\", \"p-value\", \"Signif\"),\n        align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nLag\n# of matches\nACF\nStatistic\np-value\nSignif\n\n\n\n\n4 hours\n4029\n0.2067829\n12.8553487\n0.0000000\n*\n\n\n8 hours\n3892\n0.0627710\n3.9154346\n0.0000902\n*\n\n\n12 hours\n3775\n0.0376458\n2.2875147\n0.0221658\n*\n\n\n16 hours\n3717\n0.0441777\n2.6749450\n0.0074742\n*\n\n\n20 hours\n3701\n0.0192331\n1.1676549\n0.2429460\n\n\n\n1 days\n3679\n0.0143456\n0.8608693\n0.3893100\n\n\n\n1 days 4 hours\n3666\n0.0342104\n2.0665718\n0.0387745\n*\n\n\n1 days 8 hours\n3640\n0.0239222\n1.4424779\n0.1491676\n\n\n\n1 days 12 hours\n3641\n0.0183982\n1.1013841\n0.2707295\n\n\n\n1 days 16 hours\n3631\n0.0348776\n2.1193911\n0.0340574\n*\n\n\n1 days 20 hours\n3637\n0.0263181\n1.5853192\n0.1128938\n\n\n\n2 days\n3629\n0.0097276\n0.5798249\n0.5620327\n\n\n\n2 days 4 hours\n3625\n-0.0042347\n-0.2536235\n0.7997864\n\n\n\n2 days 8 hours\n3611\n0.0034827\n0.2099082\n0.8337393\n\n\n\n2 days 12 hours\n3603\n0.0027213\n0.1635272\n0.8701034\n\n\n\n2 days 16 hours\n3594\n0.0392298\n2.3527371\n0.0186358\n*\n\n\n2 days 20 hours\n3591\n0.0094671\n0.5693210\n0.5691383\n\n\n\n3 days\n3582\n-0.0035745\n-0.2129780\n0.8313441\n\n\n\n3 days 4 hours\n3561\n-0.0005893\n-0.0350290\n0.9720566\n\n\n\n3 days 8 hours\n3544\n0.0307172\n1.8476203\n0.0646573\n\n\n\n3 days 12 hours\n3533\n0.0198054\n1.1823120\n0.2370819\n\n\n\n3 days 16 hours\n3529\n-0.0162434\n-0.9594385\n0.3373379\n\n\n\n3 days 20 hours\n3517\n0.0094346\n0.5583253\n0.5766223\n\n\n\n4 days\n3512\n0.0423126\n2.4940588\n0.0126292\n*\n\n\n4 days 4 hours\n3502\n0.0088230\n0.5245023\n0.5999292\n\n\n\n4 days 8 hours\n3496\n0.0029613\n0.1756917\n0.8605362\n\n\n\n4 days 12 hours\n3492\n0.0139603\n0.8234527\n0.4102506\n\n\n\n4 days 16 hours\n3488\n-0.0026395\n-0.1548994\n0.8769006\n\n\n\n4 days 20 hours\n3484\n0.0268817\n1.5974640\n0.1101623\n\n\n\n5 days\n3463\n0.0238308\n1.4094495\n0.1587023\n\n\n\n5 days 4 hours\n3448\n-0.0157232\n-0.9239779\n0.3554978\n\n\n\n5 days 8 hours\n3444\n-0.0183790\n-1.0771075\n0.2814322\n\n\n\n5 days 12 hours\n3432\n-0.0419971\n-2.4461672\n0.0144384\n*\n\n\n5 days 16 hours\n3428\n-0.0101644\n-0.5981104\n0.5497663\n\n\n\n5 days 20 hours\n3421\n-0.0298322\n-1.7470112\n0.0806354\n\n\n\n6 days\n3415\n-0.0259459\n-1.5203068\n0.1284339\n\n\n\n6 days 4 hours\n3400\n-0.0266511\n-1.5626079\n0.1181448\n\n\n\n6 days 8 hours\n3387\n0.0028246\n0.1640629\n0.8696816\n\n\n\n6 days 12 hours\n3376\n-0.0024564\n-0.1444242\n0.8851655\n\n\n\n6 days 16 hours\n3371\n-0.0164474\n-0.9571512\n0.3384909\n\n\n\n6 days 20 hours\n3369\n0.0005433\n0.0313962\n0.9749536\n\n\n\n7 days\n3370\n-0.0151142\n-0.8721895\n0.3831050\n\n\n\n\n\n\nCode\nACF %&gt;%\n  mutate(signif = ifelse(p.value &lt; 0.05, \"Significant\", \"Not significant\")) %&gt;% \n  ggplot(mapping = aes(x = lag, y = acf)) +\n  geom_hline(yintercept = 0) +\n  geom_segment(mapping = aes(\n    xend = lag, \n    yend = 0,\n    colour = signif)) +\n  scale_colour_manual(values = c(\"black\", \"red\")) +\n  theme_test() +\n  labs(\n    x = \"Lag\",\n    y = \"ACF\",\n    colour = \"p-value &lt; 0.05\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Analisi del primo elefante (id: GON09)</span>"
    ]
  },
  {
    "objectID": "code/GON09.html#modello-ar1",
    "href": "code/GON09.html#modello-ar1",
    "title": "4  Analisi del primo elefante (id: GON09)",
    "section": "4.3 Modello AR(1)",
    "text": "4.3 Modello AR(1)\nDato che dalla funzione di autocorrelazione emerge una correlazione significativa dei primi 4 lag, cioè i residui al tempo \\(t-1\\) (4 ore), \\(t-2\\) (8 ore), \\(t-3\\) (12 ore), \\(t-4\\) (16 ore) sono correlati con i residui al tempo \\(t\\), e non è quindi possibile assumere indipendenza tra le osservazioni, la funzione di log-verosimiglianza è ridefinita per tenerne conto come segue:\n\nSiano:\n\\(\\eta = \\mathbf{X}\\boldsymbol{\\beta}\\): il predittore lineare\n\\(e_{t-1} = y_{t-1} - 2 \\cdot \\arctan(\\eta_{t-1})\\): i residui al tempo t-1\nAssumiamo:\n\n\\[\n\\mu_t = 2 \\cdot \\arctan(\\eta_{t}) + \\arctan(\\frac{\\phi \\cdot \\sin(e_{t-1})}{k_t})\n\\]\ne\n\\[\nk_t = \\sqrt{k^2 + [\\phi \\cdot \\sin(e_{t-1})]^2}\n\\]\n\n\nCode\n# funzione di verosimiglianza ----\npseudo.log.lik.VM.ar &lt;- function(par, data, formula, response, burst) {\n  # dati ----\n  X &lt;- model.matrix(formula, data)\n  y &lt;- data[[response]]\n  burst &lt;- data[[burst]]\n  \n  # parametri ----\n  beta &lt;- par[1:ncol(X)]\n  phi &lt;- par[ncol(X) + 1]\n  kappa &lt;- exp(par[ncol(X) + 2])\n  \n  # funzione link ----\n  eta &lt;- X %*% beta\n  l_data &lt;- tibble(\n    burst = burst,\n    y = y,\n    eta = as.vector(eta)\n  ) %&gt;% \n    group_by(burst) %&gt;% \n    mutate(\n      res = y - 2 * atan(eta),\n      kappa_t = sqrt(kappa^2 + (phi * sin(lag(res)))^2),\n      mu_t = 2 * atan(eta) + atan(phi * sin(lag(res)) / kappa_t),\n      l = kappa_t * cos(y - mu_t) - log(besselI(kappa_t, nu = 0))\n    )\n  \n  # log-likleihood function ----\n  return(-sum(l_data$l, na.rm = T))\n}\n# fit ----\nfit_pmle &lt;- optim(\n  par = c(rnorm(7, sd = 1), log(2)),\n  fn =  pseudo.log.lik.VM.ar,\n  data = GON09_fnl,\n  formula = ~ distriv_std + elev_std + ndvi_std ,\n  response = \"ta_\",\n  burst = \"burst_\",\n  method = \"L-BFGS-B\",\n  hessian = TRUE,\n  control = list(maxit = 1000)\n)\n# Risultati \ntibble(\n  parameters = c(\"Intercept\", \"Distance from water\", \"Elevation\", \"NDVI Index\", \"Seas = CD\", \"Seas = HD\", \"phi\", \"kappa\"),\n  estimate = c(fit_pmle$par[1:7], exp(fit_pmle$par[8])), \n  # se = sqrt(diag(solve(fit_pmle$hessian))),\n  # lower = estimate - qnorm(0.975) * se,\n  # upper = estimate + qnorm(0.975) * se,\n  # W = estimate / se, \n  # p_value = 2 * (1 - pnorm(abs(W)))\n) %&gt;% \n  mutate(across(where(is.numeric), ~ round(.x, 4))) %&gt;%\n  kable(\n    col.names = c(\"Parameter\", \"Estimate\"),\n    align = \"lcccc\",\n    format = \"markdown\"\n  )\n\n\n\n\n\nParameter\nEstimate\n\n\n\n\nIntercept\n-0.0381\n\n\nDistance from water\n0.0326\n\n\nElevation\n0.0047\n\n\nNDVI Index\n0.0342\n\n\nSeas = CD\n0.5394\n\n\nSeas = HD\n-0.2498\n\n\nphi\n1.3425\n\n\nkappa\n2.0000",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Analisi del primo elefante (id: GON09)</span>"
    ]
  }
]